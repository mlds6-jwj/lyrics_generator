{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LvC917aDZmiw"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "    %load_ext tensorboard\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, random\n",
        "\n",
        "tf.random.set_seed(123)\n",
        "np.random.seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_corpus(path):\n",
        "  \"\"\"\n",
        "  path: Path relativo a este script\n",
        "  Esta función carga el corpus para su procesamiento y lectura en la red neuronal\n",
        "  \"\"\"\n",
        "  with open(path) as f:\n",
        "    corpus = f.readlines()\n",
        "    corpus = ' '.join(corpus)\n",
        "  return corpus"
      ],
      "metadata": {
        "id": "s9fX0wRQayml"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorized_text(corpus):\n",
        "  \"\"\"\n",
        "  corpus: str, texto completo a usar para entrenar el modelo.\n",
        "  Función que recibe un texto y lo devuelve en una representación númerica.\n",
        "  \"\"\"\n",
        "  \n",
        "  #Vocabulario, caracteres unicos que aparecen en el corpus\n",
        "  vocab = sorted(np.unique(list(set(corpus))))\n",
        "  #Asignar un valor numerico a cada caracter\n",
        "  char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "  #letra asignada al vocab\n",
        "  idx2char = np.array(vocab)\n",
        "\n",
        "  text_as_int = np.array([char2idx[c] for c in corpus])\n",
        "\n",
        "  return text_as_int, idx2char"
      ],
      "metadata": {
        "id": "Ohwh7FiYmGxD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "a-j9PKFOguE_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_preparation(text_as_int, len_corpus, seq_length=100, buffer_size = 10000, batch_size = 64):\n",
        "  examples_per_epoch = len_corpus // (seq_length + 1) \n",
        "  char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "  sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "  dataset = sequences.map(split_input_target).shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "  return examples_per_epoch, dataset"
      ],
      "metadata": {
        "id": "-Dq9lVZreilk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(dataset):\n",
        "  test_dataset = dataset.take(int(len(list(dataset))*0.2))\n",
        "  train_dataset = dataset.skip(int(len(list(dataset))*0.2))\n",
        "  return (test_dataset, train_dataset)"
      ],
      "metadata": {
        "id": "se1k7ymnqVAB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "5glezHXZiWB4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_definition(vocab, embedding_dim = 256, rnn_units = 1024, buffer_size = 10000, batch_size = 64, recurrent_initializer = 'glorot_uniform', dense_dim = 128, activation = \"relu\", dropout = 0.2):\n",
        "  model = tf.keras.Sequential([tf.keras.layers.Embedding(len(vocab), embedding_dim,\n",
        "                                                            batch_input_shape=[batch_size, None]),\n",
        "                                  tf.keras.layers.LSTM(rnn_units,\n",
        "                                                       return_sequences=True, #este argumento hace que el modelo sea many-to-many\n",
        "                                                       stateful=True,\n",
        "                                                       recurrent_initializer = recurrent_initializer),\n",
        "                                  tf.keras.layers.Dense(dense_dim,\n",
        "                                                        activation=\"relu\"),\n",
        "                                  tf.keras.layers.Dropout(dropout),\n",
        "                                  tf.keras.layers.Dense(len(vocab))])\n",
        "  return model"
      ],
      "metadata": {
        "id": "-AubD0X4bTas"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_model(model, optimizer, loss, metrics):\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "metadata": {
        "id": "bRB-MLu6kM6v"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_model(model, train_dataset, test_dataset, epochs, examples_per_epoch, validation_steps, batch_size=64, saving_path = 'lstm.h5'):\n",
        "  checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=saving_path,\n",
        "    save_weights_only=True, save_best_only = True)\n",
        "  model.fit(train_dataset.repeat(), validation_data = test_dataset.repeat(), epochs=epochs, callbacks=[checkpoint_callback], \n",
        "                 steps_per_epoch=examples_per_epoch//batch_size, validation_steps = validation_steps)"
      ],
      "metadata": {
        "id": "cSpd338QiejM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = load_corpus('corpus.txt')"
      ],
      "metadata": {
        "id": "SuGcB3rOnPfZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_as_int, idx2char = vectorized_text(corpus)"
      ],
      "metadata": {
        "id": "1-3cP4Fpnubh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples_per_epoch, dataset =  dataset_preparation(text_as_int = text_as_int, len_corpus = len(corpus))"
      ],
      "metadata": {
        "id": "VKTqVNO4n75y"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset, train_dataset = train_test_split(dataset)"
      ],
      "metadata": {
        "id": "yoqI5cxLq3Oh"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_definition(vocab = idx2char)"
      ],
      "metadata": {
        "id": "Jdq-FbiWo6U5"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compile_model(model, optimizer = \"adam\", loss = loss, metrics = 'accuracy')"
      ],
      "metadata": {
        "id": "GxjbZFJmo-_8"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit_model(model, train_dataset, test_dataset, 1, examples_per_epoch, validation_steps = 48)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXNCYokDpNPs",
        "outputId": "e2a13ca9-a496-41f8-f0af-5d8ccf892551"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "317/317 [==============================] - 63s 191ms/step - loss: 1.5648 - accuracy: 0.5302 - val_loss: 1.3675 - val_accuracy: 0.5796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx2char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWwf0C7Ywr5m",
        "outputId": "1e0294f3-60dc-4122-f02f-3155757508ed"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['\\n', ' ', \"'\", ',', '.', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H',\n",
              "       'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U',\n",
              "       'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h',\n",
              "       'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u',\n",
              "       'v', 'w', 'x', 'y', 'z', '\\xa0', '\\u2005', '\\u200a', '\\u205f'],\n",
              "      dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ]
}